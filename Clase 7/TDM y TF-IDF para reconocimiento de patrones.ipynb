{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"http://www.udem.edu.co/\"><img src=\"img/Escudo.png\"></a>\n",
    "<h1>Reconocimiento de Patrones I y II</h1>\n",
    "<h3>2018-2</h3>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## De la teoría a la práctica: representación de textos con la matriz TDM y el esquema TF-IDF\n",
    "\n",
    "En la sesión anterior vimos, tanto desde la teoría como en la práctica, como representar textos vectorialmente, usando patrones morfosintácticos. Vimos también, desde la teoría, la representación de textos usando el esquema de pesado TF-IDF. De igual manera, vimos dos representaciones muy importantes en la actualidad en el reconocimiento de patrones en textos: la matriz de términos en documentos (TDM) y la matriz de términos en contextos (TTM), también conocida como representación de palabras en vectores semánticos o word embeddings.\n",
    "\n",
    "En la sesión de hoy aprenderemos como llevar a la práctica la representación de textos como vectores usando TF-IDF y la matriz de términos en documentos TDM (para word embedding se recomienda leer acerca de word2vec (Mikolov, T. et. al, 2013) y mirar los tutoriales de la librería <a href=\"https://radimrehurek.com/gensim/models/word2vec.html\">gensim</a> (Khosrovian, K. et. al, 2008)  para llevar esta representación a la práctica).\n",
    "\n",
    "Para realizar la representación de textos con TDM y TF-IDF, vamos a usar la librería sklearn, especificamente CountVectorizer para TDM y TfidfVectorizer para TF-IDF. La documentación de ambos se puede encontrar en los siguientes enlaces:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trabajando sobre dos bases de datos: noticias de periódico y twits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datos desde: DB.txt . . .\n",
      "Datos cargados.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Cargando los datos desde el archivo DB.txt (Noticias de periódico)\n",
    "db = open('DB.txt', 'r', encoding=\"utf-8\")\n",
    "db_data = []\n",
    "db_target = []\n",
    "print('Cargando datos desde: DB.txt . . .')\n",
    "for l in db:\n",
    "    db_data.append(l.split('\\t')[0])\n",
    "    db_target.append(int(l.split('\\t')[1][:-1]))\n",
    "db.close()\n",
    "print('Datos cargados.')\n",
    "#print(db_data[0])\n",
    "#print(db_target)  #Las clases son 1->textos positivos; 0->tetos negativos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Cargando la base de datos de Twitter\n",
    "\n",
    "#Complete el código para cargar la BD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hay desbalance en la base de datos? Recuerde que en la práctica esto hay que manejarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(len(db_data))\n",
    "#print(len(db_data_twits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se carga el modelo CountVectorizer en la variable vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vector = CountVectorizer(ngram_range=(1,2))    #Matriz de términos en documentos\n",
    "#vector = TfidfVectorizer()   #Esquema TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El modelo aprende el vocabulario desde el corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector.fit(db_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se caracterizan los documentos como vectores a través de TDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bow = vector.transform(db_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(205, 80592)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(bow[0,:])     #Analice el formato de la matriz de términos en documentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se cargan las clases (etiqueteas) de las muestras en la variable y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = db_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta aquí, ya se tiene lista la representación de los documentos a partir de la matriz de términos en documentos. Para caracterizar los documentos usando el esquema de pesado TF-IDF, simplemente se debe ir a la celda donde se cargó el modelo CountVectorizer y comentar esa línea, y descomentar la línea donde se carga el modelo TfidfVectorizer.\n",
    "\n",
    "En este punto estamos listos para entrenar y validar diferentes modelos de aprendizaje automático y hacer el reconocimiento de patrones que permita clasificar textos como positivos o negativos, tarea que se conoce como análisis de sentimientos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recordemos la teoría de varios modelos básicos de aprendizaje automático para hacer reconocimiento de patrones.\n",
    "\n",
    "Desarrollo de la teoría en clase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis de sentimientos con diferentes modelos de aprendizaje automático usando Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresión logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muestras training:  75 %\n",
      "Muestras testing:  25 %\n",
      "\n",
      "Resultados con Regresión logística (Lineal)\n",
      "\n",
      "Accuracy:  0.7925 +/- 0.0535220327357\n",
      "Sensitivity:  0.83857344296 +/- 0.0727854359966\n",
      "Especificity:  0.743977636036 +/- 0.0798106218797\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def error_measures(Yestimado, Yteorico):\n",
    "    \n",
    "    CM = confusion_matrix(Yteorico, Yestimado)\n",
    "\n",
    "    TN = CM[0][0]\n",
    "    FN = CM[1][0]\n",
    "    TP = CM[1][1]\n",
    "    FP = CM[0][1]\n",
    "    \n",
    "    sens = TP/(TP+FN)\n",
    "    esp = TN/(TN+FP)\n",
    "    \n",
    "    return sens, esp\n",
    "\n",
    "lr=LogisticRegression()\n",
    "acc = []\n",
    "sens = []\n",
    "esp = []\n",
    "\n",
    "for i in range(100):\n",
    "    \n",
    "    Xtrain,Xtest,Ytrain,Ytest = train_test_split(bow,y)   #Realiza una única partición\n",
    "                                                          #de la base de datos.\n",
    "    \n",
    "    lr.fit(Xtrain,Ytrain)\n",
    "    Yest = lr.predict(Xtest)\n",
    "    s, e = error_measures(Yest,Ytest)\n",
    "    sens.append(s); esp.append(e)\n",
    "    acc.append(lr.score(Xtest,Ytest))\n",
    "\n",
    "print(\"Muestras training: \", round((np.size(Xtrain,0)*100)/205), \"%\")\n",
    "print(\"Muestras testing: \", round((np.size(Xtest,0)*100)/205), \"%\")\n",
    "    \n",
    "print(\"\\nResultados con Regresión logística (Lineal)\\n\")\n",
    "print(\"Accuracy: \", np.mean(acc), \"+/-\", np.std(acc))\n",
    "print(\"Sensitivity: \", np.mean(sens), \"+/-\", np.std(sens))\n",
    "print(\"Especificity: \", np.mean(esp), \"+/-\", np.std(esp))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicios\n",
    "\n",
    "1. Complete las siguientes celdas de código para probar el reconocimiento de patrones (sentiment analysis) con los modelos KNN, árboles de decisión, Random Forest y ANN (multi-layer perceptron).\n",
    "\n",
    "2. Efectue el reconocimiento de patrones con los mismo modelos pero usando la base de datos de twits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K vecinos más cercanos (K-nearest neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resultados con K vecinos más cercanos\n",
      "\n",
      "Accuracy:  0.674038461538 +/- 0.0626920127412\n",
      "Sensitivity:  0.778575294111 +/- 0.0848182121972\n",
      "Especificity:  0.56102816433 +/- 0.11792876454\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "\n",
    "knn=KNN(n_neighbors=5)\n",
    "acc = []\n",
    "sens = []\n",
    "esp = []\n",
    "\n",
    "for i in range(100):\n",
    "    Xtrain,Xtest,Ytrain,Ytest = train_test_split(bow,y)\n",
    "\n",
    "    knn.fit(Xtrain, Ytrain)\n",
    "    Yest = knn.predict(Xtest)\n",
    "    s, e = error_measures(Yest,Ytest)\n",
    "    sens.append(s); esp.append(e)\n",
    "    acc.append(knn.score(Xtest,Ytest))\n",
    "\n",
    "print(\"\\nResultados con K vecinos más cercanos\\n\")\n",
    "print(\"Accuracy: \", np.mean(acc), \"+/-\", np.std(acc))\n",
    "print(\"Sensitivity: \", np.mean(sens), \"+/-\", np.std(sens))\n",
    "print(\"Especificity: \", np.mean(esp), \"+/-\", np.std(esp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Árboles de decisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados con Árbol de decisión\n",
      "\n",
      "Accuracy:  0.641923076923 +/- 0.0700158477411\n",
      "Sensitivity:  0.912173001682 +/- 0.0437040849392\n",
      "Especificity:  0.650556947884 +/- 0.0861469311295\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier as DT\n",
    "\n",
    "clf = DT(min_samples_leaf=10)\n",
    "\n",
    "acc = []\n",
    "sens = []\n",
    "esp = []\n",
    "\n",
    "for i in range(100):\n",
    "    Xtrain,Xtest,Ytrain,Ytest = train_test_split(bow,y)\n",
    "\n",
    "    clf.fit(Xtrain, Ytrain)\n",
    "    Yest = knn.predict(Xtest)\n",
    "    s, e = error_measures(Yest,Ytest)\n",
    "    sens.append(s); esp.append(e)\n",
    "    acc.append(clf.score(Xtest,Ytest))\n",
    "\n",
    "#Complete el código para Árboles de decisión\n",
    "\n",
    "print(\"Resultados con Árbol de decisión\\n\")\n",
    "print(\"Accuracy: \", np.mean(acc), \"+/-\", np.std(acc))\n",
    "print(\"Sensitivity: \", np.mean(sens), \"+/-\", np.std(sens))\n",
    "print(\"Especificity: \", np.mean(esp), \"+/-\", np.std(esp)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados con Random Forest\n",
      "\n",
      "Accuracy:  nan +/- nan\n",
      "Sensitivity:  nan +/- nan\n",
      "Especificity:  nan +/- nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2909: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:135: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  keepdims=keepdims)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:105: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:127: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "\n",
    "acc = []\n",
    "sens = []\n",
    "esp = []\n",
    "\n",
    "#Complete el código para Árboles de decisión\n",
    "\n",
    "print(\"Resultados con Random Forest\\n\")\n",
    "print(\"Accuracy: \", np.mean(acc), \"+/-\", np.std(acc))\n",
    "print(\"Sensitivity: \", np.mean(sens), \"+/-\", np.std(sens))\n",
    "print(\"Especificity: \", np.mean(esp), \"+/-\", np.std(esp))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed-Fordward Neural Networks (Multi-layer perceptron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados con Perceptróin\n",
      "\n",
      "Accuracy:  nan +/- nan\n",
      "Sensitivity:  nan +/- nan\n",
      "Especificity:  nan +/- nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2909: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:135: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  keepdims=keepdims)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:105: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:127: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import Perceptron as MLP\n",
    "\n",
    "acc = []\n",
    "sens = []\n",
    "esp = []\n",
    "\n",
    "#Complete el código para ANN (Multilayer perceptron)\n",
    "\n",
    "print(\"Resultados con Perceptróin\\n\")\n",
    "print(\"Accuracy: \", np.mean(acc), \"+/-\", np.std(acc))\n",
    "print(\"Sensitivity: \", np.mean(sens), \"+/-\", np.std(sens))\n",
    "print(\"Especificity: \", np.mean(esp), \"+/-\", np.std(esp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Referencias\n",
    "\n",
    "Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.\n",
    "\n",
    "Khosrovian, K., Pfahl, D., & Garousi, V. (2008, May). GENSIM 2.0: a customizable process simulation model for software process evaluation. In International Conference on Software Process (pp. 294-306). Springer, Berlin, Heidelberg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
